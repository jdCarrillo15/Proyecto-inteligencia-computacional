"""
Script de entrenamiento del modelo de clasificaci√≥n de enfermedades.

Este script:
1. Prepara los datos autom√°ticamente (con cache PKL)
2. Entrena el modelo con Transfer Learning
3. Eval√∫a y guarda resultados

Uso:
    python backend/scripts/train.py
    
El sistema detecta autom√°ticamente si necesita preparar datos o puede
usar el cache existente.
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import sys
import time

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, top_k_accuracy_score
from datetime import datetime

# Agregar el directorio backend al path
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from utils.data_cache import DataCache

# Configurar para mejor rendimiento
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reducir logs de TensorFlow
tf.random.set_seed(42)
np.random.seed(42)

# Optimizaciones de TensorFlow
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print("‚úÖ GPU detectada y configurada")


class MetricsLogger(Callback):
    """
    Callback personalizado para loggear m√©tricas detalladas cada epoch.
    Calcula F1-score, per-crop accuracy y otras m√©tricas avanzadas.
    """
    
    def __init__(self, X_val, y_val, class_names, phase_name="Training"):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.class_names = class_names
        self.phase_name = phase_name
        self.epoch_metrics = []
        
        # Mapear clases a cultivos
        self.crop_mapping = self._create_crop_mapping()
    
    def _create_crop_mapping(self):
        """Mapea cada clase a su cultivo."""
        mapping = {}
        for i, class_name in enumerate(self.class_names):
            if 'Apple' in class_name:
                mapping[i] = 'Apple'
            elif 'Corn' in class_name or 'maize' in class_name:
                mapping[i] = 'Corn'
            elif 'Potato' in class_name:
                mapping[i] = 'Potato'
            elif 'Tomato' in class_name:
                mapping[i] = 'Tomato'
            else:
                mapping[i] = 'Other'
        return mapping
    
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        
        # Predicciones
        y_pred = self.model.predict(self.X_val, verbose=0)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_true_classes = np.argmax(self.y_val, axis=1)
        
        # Calcular F1-score macro
        _, _, f1_scores, _ = precision_recall_fscore_support(
            y_true_classes, y_pred_classes, average='macro', zero_division=0
        )
        
        # Calcular per-crop accuracy
        crop_accuracies = {}
        for crop in ['Apple', 'Corn', 'Potato', 'Tomato']:
            crop_indices = [i for i, c in self.crop_mapping.items() if c == crop]
            if crop_indices:
                mask = np.isin(y_true_classes, crop_indices)
                if mask.sum() > 0:
                    crop_acc = (y_true_classes[mask] == y_pred_classes[mask]).mean()
                    crop_accuracies[crop] = crop_acc
        
        # Guardar m√©tricas
        epoch_data = {
            'epoch': epoch + 1,
            'train_loss': logs.get('loss', 0),
            'train_acc': logs.get('accuracy', 0),
            'val_loss': logs.get('val_loss', 0),
            'val_acc': logs.get('val_accuracy', 0),
            'val_f1': f1_scores,
            'crop_acc': crop_accuracies
        }
        self.epoch_metrics.append(epoch_data)
        
        # Imprimir resumen
        print(f"\nüìã [{self.phase_name}] M√©tricas adicionales:")
        print(f"  - Val F1 (macro): {f1_scores:.4f}")
        if crop_accuracies:
            crop_str = ", ".join([f"{crop}={acc:.2%}" for crop, acc in crop_accuracies.items()])
            print(f"  - Per-Crop: {crop_str}")


class FineTuningMonitor(Callback):
    """
    Callback personalizado para monitorear se√±ales de √©xito y problemas durante fine-tuning.
    
    Se√±ales de √©xito:
    - ‚úÖ Val accuracy sube gradualmente
    - ‚úÖ Val loss baja sin oscilar mucho
    - ‚úÖ Gap train-val no es muy grande (<10%)
    
    Se√±ales de problemas:
    - ‚ùå Val loss explota ‚Üí LR demasiado alto
    - ‚ùå Overfitting severo (train 95%, val 50%) ‚Üí M√°s regularizaci√≥n
    - ‚ùå No mejora nada ‚Üí Posible problema en datos
    """
    
    def __init__(self, phase_name="Fine-tuning"):
        super().__init__()
        self.phase_name = phase_name
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.epochs_no_improve = 0
        self.val_loss_history = []
        
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        
        val_loss = logs.get('val_loss', 0)
        val_acc = logs.get('val_accuracy', 0)
        train_loss = logs.get('loss', 0)
        train_acc = logs.get('accuracy', 0)
        
        # Calcular gap train-val
        acc_gap = abs(train_acc - val_acc)
        
        # Guardar historial
        self.val_loss_history.append(val_loss)
        
        # Detectar volatilidad en val_loss
        if len(self.val_loss_history) >= 3:
            recent_losses = self.val_loss_history[-3:]
            volatility = max(recent_losses) - min(recent_losses)
        else:
            volatility = 0
        
        print(f"\nüìä [{self.phase_name}] Epoch {epoch + 1} - Monitoreo:")
        
        # SE√ëALES DE √âXITO
        success_signals = []
        
        if val_acc > self.best_val_acc:
            improvement = (val_acc - self.best_val_acc) * 100
            success_signals.append(f"‚úÖ Val accuracy mejora: +{improvement:.2f}%")
            self.best_val_acc = val_acc
            self.epochs_no_improve = 0
        
        if val_loss < self.best_val_loss:
            success_signals.append(f"‚úÖ Val loss baja: {val_loss:.4f}")
            self.best_val_loss = val_loss
        
        if acc_gap < 0.10:
            success_signals.append(f"‚úÖ Gap train-val saludable: {acc_gap*100:.1f}%")
        
        if volatility < 0.2:
            success_signals.append("‚úÖ Val loss estable (baja oscilaci√≥n)")
        
        # SE√ëALES DE PROBLEMAS
        problem_signals = []
        
        # Val loss explota
        if len(self.val_loss_history) >= 2:
            if val_loss > self.val_loss_history[-2] * 1.5:
                problem_signals.append("‚ùå ALERTA: Val loss explota - LR puede ser muy alto")
        
        # Overfitting severo
        if train_acc > 0.95 and val_acc < 0.70:
            problem_signals.append(f"‚ùå OVERFITTING SEVERO: train={train_acc:.1%}, val={val_acc:.1%}")
        elif acc_gap > 0.15:
            problem_signals.append(f"‚ö†Ô∏è  Gap train-val alto: {acc_gap*100:.1f}% (>15%)")
        
        # Estancamiento
        if val_acc <= self.best_val_acc:
            self.epochs_no_improve += 1
            if self.epochs_no_improve >= 5:
                problem_signals.append(f"‚ö†Ô∏è  Sin mejora por {self.epochs_no_improve} epochs")
        
        # Volatilidad alta
        if volatility > 0.3:
            problem_signals.append(f"‚ö†Ô∏è  Val loss oscila mucho: volatilidad={volatility:.3f}")
        
        # Imprimir se√±ales
        if success_signals:
            print("  " + "\n  ".join(success_signals))
        
        if problem_signals:
            print("  " + "\n  ".join(problem_signals))
        
        if not success_signals and not problem_signals:
            print("  üîµ Entrenamiento en progreso normal")
        
        # M√©tricas actuales
        print(f"  üìã M√©tricas: train_acc={train_acc:.1%}, val_acc={val_acc:.1%}, gap={acc_gap*100:.1f}%")


class PlantDiseaseClassifier:
    """Clasificador de enfermedades de plantas con Transfer Learning."""
    
    def __init__(self, img_size=(100, 100), num_classes=15, use_transfer_learning=True):
        """
        Inicializa el clasificador.
        
        Args:
            img_size: Tama√±o de las im√°genes
            num_classes: N√∫mero de clases (15 enfermedades espec√≠ficas por defecto)
            use_transfer_learning: Usar MobileNetV2 pre-entrenado
        """
        self.img_size = img_size
        self.num_classes = num_classes
        self.use_transfer_learning = use_transfer_learning
        self.model = None
        self.history = None
        self.cache = DataCache()
    
    def build_model(self):
        """Construye el modelo optimizado."""
        
        if self.use_transfer_learning:
            print("\nüöÄ Construyendo modelo con MobileNetV2 (Transfer Learning)")
            
            # Data augmentation
            data_augmentation = keras.Sequential([
                layers.RandomFlip("horizontal"),
                layers.RandomRotation(0.15),
                layers.RandomZoom(0.15),
                layers.RandomContrast(0.1),
            ], name="data_augmentation")
            
            # Modelo base pre-entrenado
            base_model = keras.applications.MobileNetV2(
                input_shape=(self.img_size[0], self.img_size[1], 3),
                include_top=False,
                weights='imagenet'
            )
            
            # Congelar base model
            base_model.trainable = False
            
            # Construir modelo con augmentation
            inputs = keras.Input(shape=(self.img_size[0], self.img_size[1], 3))
            x = data_augmentation(inputs)
            x = keras.applications.mobilenet_v2.preprocess_input(x)
            x = base_model(x, training=False)
            x = layers.GlobalAveragePooling2D()(x)
            x = layers.Dropout(0.3)(x)
            x = layers.Dense(256, activation='relu')(x)
            x = layers.Dropout(0.3)(x)
            outputs = layers.Dense(self.num_classes, activation='softmax')(x)
            
            model = keras.Model(inputs, outputs)
            self.base_model = base_model
            self.data_augmentation = data_augmentation
            
        else:
            print("\nüî® Construyendo CNN desde cero")
            
            model = models.Sequential([
                layers.Conv2D(32, (3, 3), activation='relu', 
                            input_shape=(self.img_size[0], self.img_size[1], 3)),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                layers.Conv2D(64, (3, 3), activation='relu'),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                layers.Conv2D(128, (3, 3), activation='relu'),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                layers.Flatten(),
                layers.Dense(256, activation='relu'),
                layers.Dropout(0.5),
                layers.Dense(self.num_classes, activation='softmax')
            ])
        
        # Compilar con learning rate ajustado
        initial_lr = 0.001 if self.use_transfer_learning else 0.0005
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=initial_lr),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        self.model = model
        
        print(f"\n‚úÖ Modelo construido: {model.count_params():,} par√°metros")
        return model
    
    def train_with_arrays(self, X_train, y_train, X_test, y_test, 
                         class_names, epochs=20, batch_size=64):
        """
        Entrena el modelo con arrays numpy (datos desde cache).
        
        Args:
            X_train: Array de im√°genes de entrenamiento
            y_train: Labels de entrenamiento (one-hot)
            X_test: Array de im√°genes de prueba
            y_test: Labels de prueba (one-hot)
            class_names: Nombres de las clases
            epochs: N√∫mero de √©pocas
            batch_size: Tama√±o del batch
            
        Returns:
            History object
        """
        print("\n" + "=" * 60)
        print("üéØ INICIANDO ENTRENAMIENTO")
        print("=" * 60)
        print(f"  - Muestras train: {len(X_train):,}")
        print(f"  - Muestras test: {len(X_test):,}")
        print(f"  - Batch size: {batch_size}")
        print(f"  - √âpocas: {epochs}")
        print()
        
        # Crear directorio para modelos
        Path('models').mkdir(exist_ok=True)
        
        # Callbacks optimizados para Fase 1
        callbacks = [
            MetricsLogger(X_test, y_test, class_names, phase_name="Fase 1"),
            EarlyStopping(
                monitor='val_accuracy',
                patience=7,
                restore_best_weights=True,
                verbose=1
            ),
            ModelCheckpoint(
                'models/best_model.keras',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,          # Decay agresivo para convergencia r√°pida
                patience=3,          # Reacci√≥n r√°pida a estancamiento
                min_lr=0.0001,       # M√≠nimo para Fase 1
                verbose=1
            )
        ]
        
        # Entrenar
        start_time = time.time()
        
        self.history = self.model.fit(
            X_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(X_test, y_test),
            callbacks=callbacks,
            verbose=1
        )
        
        training_time = time.time() - start_time
        
        print(f"\n‚è±Ô∏è  Tiempo de entrenamiento: {training_time/60:.2f} minutos")
        
        return self.history
    
    def fine_tune(self, X_train, y_train, X_test, y_test, 
                  class_names, epochs_phase2=15, batch_size=64):
        """
        Fine-tuning del modelo con descongelamiento gradual (solo si usa transfer learning).
        
        Estrategia basada en an√°lisis de features de MobileNetV2:
        - Capas 0-50:   Features b√°sicas (bordes, texturas) ‚Üí MANTENER CONGELADAS
        - Capas 51-100: Features intermedias (patrones) ‚Üí Fase 2b
        - Capas 101-154: Features complejas (objetos) ‚Üí Fase 2a (m√°s relevantes para hojas)
        
        Fase 2a: Descongela capas 101-154 (features complejas)
        Fase 2b: Descongela capas 51-154 (a√±ade features intermedias)
        
        Nota: BatchNormalization layers se manejan cuidadosamente con batch_size peque√±o.
        
        Args:
            X_train, y_train: Datos de entrenamiento
            X_test, y_test: Datos de prueba
            class_names: Nombres de las clases
            epochs_phase2: √âpocas totales de fine-tuning (se divide en 2 subfases)
            batch_size: Tama√±o del batch
        """
        if not self.use_transfer_learning:
            print("‚ö†Ô∏è  Fine-tuning solo disponible con transfer learning")
            return
        
        total_layers = len(self.base_model.layers)
        print(f"\nüìä Base model tiene {total_layers} capas totales")
        
        # ==================================================================
        # FASE 2a: Descongelamiento de features complejas (capas 101-154)
        # ==================================================================
        print("\n" + "=" * 60)
        print("üî• FASE 2a: FINE-TUNING - Features Complejas (Capas 101-154)")
        print("=" * 60)
        print("  üåø Objetivo: Adaptar detecci√≥n de objetos completos a morfolog√≠a de hojas")
        
        # Descongelar base model
        self.base_model.trainable = True
        
        # Estrategia: Descongelar solo capas 101-154 (features complejas)
        # Mantener congeladas 0-100 (features b√°sicas e intermedias)
        fine_tune_at = min(101, total_layers - 1)
        
        for i, layer in enumerate(self.base_model.layers):
            if i < fine_tune_at:
                layer.trainable = False
            else:
                # Proteger BatchNormalization con batch_size peque√±o
                if 'BatchNormalization' in layer.__class__.__name__ and batch_size < 16:
                    layer.trainable = False
                else:
                    layer.trainable = True
        
        trainable_layers = sum([1 for layer in self.base_model.layers if layer.trainable])
        frozen_bn = sum([1 for layer in self.base_model.layers[fine_tune_at:] 
                        if 'BatchNormalization' in layer.__class__.__name__ and not layer.trainable])
        
        print(f"  - Rango de capas: {fine_tune_at}-{total_layers} (features complejas)")
        print(f"  - Capas congeladas: 0-{fine_tune_at-1} (features b√°sicas/intermedias)")
        print(f"  - Capas descongeladas: {trainable_layers}")
        if frozen_bn > 0:
            print(f"  - BatchNorm protegidas: {frozen_bn} (batch_size={batch_size} < 16)")
        print(f"  - Learning Rate: 0.0001 (10x m√°s bajo que Fase 1)")
        print(f"  - LR Decay: factor=0.2, patience=5, min_lr=0.00001")
        
        # Recompilar con LR bajo
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.0001),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # Callbacks para Fase 2a (fine-tuning conservador)
        callbacks_2a = [
            MetricsLogger(X_test, y_test, class_names, phase_name="Fase 2a"),
            FineTuningMonitor(phase_name="Fase 2a"),
            EarlyStopping(
                monitor='val_accuracy',
                patience=6,
                restore_best_weights=True,
                verbose=1
            ),
            ModelCheckpoint(
                'models/finetuned_phase2a.keras',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,          # Decay suave para evitar olvido catastr√≥fico
                patience=5,          # M√°s paciente en fine-tuning
                min_lr=0.00001,      # M√≠nimo para Fase 2
                verbose=1
            )
        ]
        
        # Entrenar Fase 2a
        epochs_2a = max(epochs_phase2 // 2, 7)  # M√≠nimo 7 epochs
        start_time_2a = time.time()
        
        history_2a = self.model.fit(
            X_train, y_train,
            batch_size=batch_size,
            epochs=epochs_2a,
            validation_data=(X_test, y_test),
            callbacks=callbacks_2a,
            verbose=1
        )
        
        time_2a = time.time() - start_time_2a
        print(f"\n‚è±Ô∏è  Tiempo Fase 2a: {time_2a/60:.2f} minutos")
        
        # ==================================================================
        # FASE 2b: Descongelamiento de features intermedias (capas 51-154)
        # ==================================================================
        print("\n" + "=" * 60)
        print("üî• FASE 2b: FINE-TUNING - Features Intermedias (Capas 51-154)")
        print("=" * 60)
        print("  üçÉ Objetivo: Adaptar detecci√≥n de patrones/formas a s√≠ntomas de enfermedades")
        
        # Estrategia: Descongelar capas 51-154 (features intermedias + complejas)
        # Mantener congeladas 0-50 (features b√°sicas: bordes, texturas)
        fine_tune_at_2b = min(51, total_layers - 1)
        
        for i, layer in enumerate(self.base_model.layers):
            if i < fine_tune_at_2b:
                layer.trainable = False
            else:
                # Proteger BatchNormalization con batch_size peque√±o
                if 'BatchNormalization' in layer.__class__.__name__ and batch_size < 16:
                    layer.trainable = False
                else:
                    layer.trainable = True
        
        trainable_layers_2b = sum([1 for layer in self.base_model.layers if layer.trainable])
        frozen_bn_2b = sum([1 for layer in self.base_model.layers[fine_tune_at_2b:] 
                           if 'BatchNormalization' in layer.__class__.__name__ and not layer.trainable])
        
        print(f"  - Rango de capas: {fine_tune_at_2b}-{total_layers} (features intermedias/complejas)")
        print(f"  - Capas congeladas: 0-{fine_tune_at_2b-1} (features b√°sicas preservadas)")
        print(f"  - Capas descongeladas: {trainable_layers_2b}")
        if frozen_bn_2b > 0:
            print(f"  - BatchNorm protegidas: {frozen_bn_2b} (batch_size={batch_size} < 16)")
        print(f"  - Learning Rate: 0.00005 (ultra-bajo para evitar catastrophic forgetting)")
        print(f"  - LR Decay: factor=0.2, patience=5, min_lr=0.00001")
        
        # Recompilar con LR muy bajo
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.00005),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # Callbacks para Fase 2b (fine-tuning ultra-conservador)
        callbacks_2b = [
            MetricsLogger(X_test, y_test, class_names, phase_name="Fase 2b"),
            FineTuningMonitor(phase_name="Fase 2b"),
            EarlyStopping(
                monitor='val_accuracy',
                patience=5,
                restore_best_weights=True,
                verbose=1
            ),
            ModelCheckpoint(
                'models/best_model.keras',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,          # Decay suave para preservar features ImageNet
                patience=5,          # M√°s paciente con m√°s capas descongeladas
                min_lr=0.00001,      # M√≠nimo para Fase 2
                verbose=1
            )
        ]
        
        # Entrenar Fase 2b
        epochs_2b = epochs_phase2 - epochs_2a
        start_time_2b = time.time()
        
        history_2b = self.model.fit(
            X_train, y_train,
            batch_size=batch_size,
            epochs=epochs_2b,
            validation_data=(X_test, y_test),
            callbacks=callbacks_2b,
            verbose=1
        )
        
        time_2b = time.time() - start_time_2b
        print(f"\n‚è±Ô∏è  Tiempo Fase 2b: {time_2b/60:.2f} minutos")
        
        # Combinar historiales
        for key in self.history.history.keys():
            if key in history_2a.history:
                self.history.history[key].extend(history_2a.history[key])
            if key in history_2b.history:
                self.history.history[key].extend(history_2b.history[key])
        
        total_ft_time = time_2a + time_2b
        print(f"\n‚è±Ô∏è  Tiempo total de fine-tuning: {total_ft_time/60:.2f} minutos")
        print(f"\n‚úÖ Fine-tuning completado con {trainable_layers_2b} capas entrenables")
    
    def evaluate(self, X_test, y_test, class_names):
        """
        Evaluaci√≥n exhaustiva del modelo con m√©tricas detalladas.
        
        Args:
            X_test: Datos de prueba
            y_test: Labels de prueba (one-hot)
            class_names: Nombres de las clases
        """
        print("\n" + "=" * 60)
        print("üìä EVALUACI√ìN DETALLADA DEL MODELO")
        print("=" * 60)
        
        # Evaluar
        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=0)
        
        # Predicciones
        print("\nüîÆ Generando predicciones...")
        predictions = self.model.predict(X_test, verbose=0)
        predicted_classes = np.argmax(predictions, axis=1)
        true_classes = np.argmax(y_test, axis=1)
        
        # M√©tricas b√°sicas
        print(f"\n‚úÖ M√âTRICAS GLOBALES:")
        print(f"  - Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
        print(f"  - Loss: {test_loss:.4f}")
        
        # Top-K Accuracy
        top3_acc = top_k_accuracy_score(true_classes, predictions, k=3)
        top5_acc = top_k_accuracy_score(true_classes, predictions, k=5)
        print(f"  - Top-3 Accuracy: {top3_acc:.4f} ({top3_acc*100:.2f}%)")
        print(f"  - Top-5 Accuracy: {top5_acc:.4f} ({top5_acc*100:.2f}%)")
        
        # M√©tricas por clase
        per_class_metrics = self._calculate_per_class_metrics(
            true_classes, predicted_classes, class_names
        )
        
        # M√©tricas por cultivo
        per_crop_metrics = self._calculate_per_crop_metrics(
            true_classes, predicted_classes, class_names
        )
        
        # Healthy vs Diseased
        healthy_diseased_metrics = self._calculate_healthy_vs_diseased(
            true_classes, predicted_classes, class_names
        )
        
        # Matriz de confusi√≥n
        cm = confusion_matrix(true_classes, predicted_classes)
        
        # An√°lisis de confusiones
        top_confusions = self._analyze_confusions(cm, class_names)
        
        # Visualizaciones
        viz_path = Path('models/visualizations')
        viz_path.mkdir(parents=True, exist_ok=True)
        
        self._plot_confusion_matrix_detailed(cm, class_names, viz_path)
        self._plot_per_class_metrics(per_class_metrics, viz_path)
        self._plot_per_crop_performance(per_crop_metrics, viz_path)
        self._plot_healthy_vs_diseased(healthy_diseased_metrics, viz_path)
        
        # Generar reporte detallado
        self._generate_detailed_report(
            test_accuracy, test_loss, top3_acc, top5_acc,
            per_class_metrics, per_crop_metrics, 
            healthy_diseased_metrics, top_confusions,
            class_names, viz_path
        )
        
        return test_loss, test_accuracy
    
    def _calculate_per_class_metrics(self, y_true, y_pred, class_names):
        """Calcula m√©tricas detalladas por clase."""
        print("\nüìä M√âTRICAS POR CLASE:")
        print("-" * 80)
        
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average=None, zero_division=0
        )
        
        # Crear DataFrame para mejor visualizaci√≥n
        metrics_data = []
        for i, class_name in enumerate(class_names):
            metrics_data.append({
                'class': class_name,
                'precision': precision[i],
                'recall': recall[i],
                'f1': f1[i],
                'support': support[i]
            })
        
        # Ordenar por F1-score
        metrics_data = sorted(metrics_data, key=lambda x: x['f1'], reverse=True)
        
        # Imprimir
        print(f"{'Clase':<35} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}")
        print("-" * 80)
        
        for item in metrics_data:
            precision_str = f"{item['precision']:.4f}"
            recall_str = f"{item['recall']:.4f}"
            f1_str = f"{item['f1']:.4f}"
            
            # Colorear seg√∫n rendimiento
            if item['f1'] < 0.6:
                indicator = "üî¥"
            elif item['f1'] < 0.8:
                indicator = "üü°"
            else:
                indicator = "üü¢"
            
            print(f"{item['class']:<35} {precision_str:<12} {recall_str:<12} {f1_str:<12} {item['support']:<10} {indicator}")
        
        # Promedios
        print("-" * 80)
        macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average='macro', zero_division=0
        )
        weighted_p, weighted_r, weighted_f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average='weighted', zero_division=0
        )
        
        print(f"{'Macro Avg':<35} {macro_p:.4f}       {macro_r:.4f}       {macro_f1:.4f}")
        print(f"{'Weighted Avg':<35} {weighted_p:.4f}       {weighted_r:.4f}       {weighted_f1:.4f}")
        
        return metrics_data
    
    def _calculate_per_crop_metrics(self, y_true, y_pred, class_names):
        """Calcula accuracy por cultivo."""
        print("\nüåæ M√âTRICAS POR CULTIVO:")
        print("-" * 50)
        
        crop_mapping = {}
        for i, class_name in enumerate(class_names):
            if 'Apple' in class_name:
                crop_mapping[i] = 'Apple'
            elif 'Corn' in class_name or 'maize' in class_name:
                crop_mapping[i] = 'Corn'
            elif 'Potato' in class_name:
                crop_mapping[i] = 'Potato'
            elif 'Tomato' in class_name:
                crop_mapping[i] = 'Tomato'
        
        crop_metrics = {}
        for crop in ['Apple', 'Corn', 'Potato', 'Tomato']:
            crop_indices = [i for i, c in crop_mapping.items() if c == crop]
            if crop_indices:
                mask = np.isin(y_true, crop_indices)
                if mask.sum() > 0:
                    correct = (y_true[mask] == y_pred[mask]).sum()
                    total = mask.sum()
                    accuracy = correct / total
                    crop_metrics[crop] = {
                        'accuracy': accuracy,
                        'correct': correct,
                        'total': total
                    }
        
        # Imprimir
        for crop, metrics in sorted(crop_metrics.items(), key=lambda x: x[1]['accuracy'], reverse=True):
            acc = metrics['accuracy']
            indicator = "üü¢" if acc > 0.8 else "üü°" if acc > 0.6 else "üî¥"
            print(f"{crop:<10} Accuracy: {acc:.4f} ({acc*100:.2f}%) - {metrics['correct']}/{metrics['total']} {indicator}")
        
        return crop_metrics
    
    def _calculate_healthy_vs_diseased(self, y_true, y_pred, class_names):
        """Calcula m√©tricas binarias: sano vs enfermo."""
        print("\nüè• AN√ÅLISIS: SANO VS ENFERMO")
        print("-" * 50)
        
        # Mapear a binario
        y_true_binary = np.array(['healthy' in class_names[i].lower() for i in y_true]).astype(int)
        y_pred_binary = np.array(['healthy' in class_names[i].lower() for i in y_pred]).astype(int)
        
        # Confusion matrix 2x2
        cm_binary = confusion_matrix(y_true_binary, y_pred_binary)
        
        # Calcular m√©tricas
        tn, fp, fn, tp = cm_binary.ravel() if cm_binary.size == 4 else (0, 0, 0, 0)
        
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        print(f"Binary Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"\\nConfusion Matrix (Binaria):")
        print(f"  Diseased‚ÜíDiseased: {tn:4d}   Diseased‚ÜíHealthy: {fp:4d} ‚ö†Ô∏è")
        print(f"  Healthy‚ÜíDiseased:  {fn:4d} ‚ö†Ô∏è  Healthy‚ÜíHealthy:  {tp:4d}")
        
        if fn > 0:
            print(f"\\n‚ö†Ô∏è  CR√çTICO: {fn} falsos negativos (enfermo clasificado como sano)")
        if fp > 0:
            print(f"‚ö†Ô∏è  {fp} falsos positivos (sano clasificado como enfermo)")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,
            'cm_binary': cm_binary
        }
    
    def _analyze_confusions(self, cm, class_names):
        """Analiza las confusiones m√°s frecuentes."""
        print("\nüîç TOP 10 CONFUSIONES:")
        print("-" * 70)
        
        confusions = []
        for i in range(len(class_names)):
            for j in range(len(class_names)):
                if i != j and cm[i, j] > 0:
                    confusions.append({
                        'true': class_names[i],
                        'pred': class_names[j],
                        'count': cm[i, j]
                    })
        
        # Ordenar por cantidad
        confusions = sorted(confusions, key=lambda x: x['count'], reverse=True)[:10]
        
        for idx, conf in enumerate(confusions, 1):
            print(f"{idx:2d}. {conf['true']:<30} ‚Üí {conf['pred']:<30} : {conf['count']:4d} veces")
        
        return confusions
    
    def _plot_confusion_matrix_detailed(self, cm, class_names, viz_path):
        """Visualiza matriz de confusi√≥n detallada con alta resoluci√≥n."""
        # Calcular matriz normalizada
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))
        
        # Matriz absoluta
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
                   xticklabels=class_names, yticklabels=class_names,
                   cbar_kws={'label': 'Cantidad'})
        ax1.set_title('Matriz de Confusi√≥n (Valores Absolutos)', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Clase Real', fontsize=12)
        ax1.set_xlabel('Clase Predicha', fontsize=12)
        ax1.tick_params(axis='x', rotation=45, labelsize=8)
        ax1.tick_params(axis='y', rotation=0, labelsize=8)
        
        # Matriz normalizada
        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='RdYlGn', ax=ax2,
                   xticklabels=class_names, yticklabels=class_names,
                   vmin=0, vmax=1, cbar_kws={'label': 'Proporci√≥n'})
        ax2.set_title('Matriz de Confusi√≥n (Normalizada por Fila)', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Clase Real', fontsize=12)
        ax2.set_xlabel('Clase Predicha', fontsize=12)
        ax2.tick_params(axis='x', rotation=45, labelsize=8)
        ax2.tick_params(axis='y', rotation=0, labelsize=8)
        
        plt.tight_layout()
        plt.savefig(viz_path / 'confusion_matrix_detailed.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ‚úÖ Matriz detallada: {viz_path / 'confusion_matrix_detailed.png'}")
    
    def _plot_per_class_metrics(self, metrics_data, viz_path):
        """Visualiza m√©tricas por clase en gr√°fico de barras."""
        fig, ax = plt.subplots(figsize=(14, 10))
        
        # Ordenar por F1-score
        metrics_data = sorted(metrics_data, key=lambda x: x['f1'], reverse=True)
        
        classes = [m['class'] for m in metrics_data]
        precision = [m['precision'] for m in metrics_data]
        recall = [m['recall'] for m in metrics_data]
        f1 = [m['f1'] for m in metrics_data]
        
        x = np.arange(len(classes))
        width = 0.25
        
        bars1 = ax.barh(x - width, precision, width, label='Precision', color='#3498db')
        bars2 = ax.barh(x, recall, width, label='Recall', color='#2ecc71')
        bars3 = ax.barh(x + width, f1, width, label='F1-Score', color='#e74c3c')
        
        # Colorear seg√∫n rendimiento
        for i, bar in enumerate(bars3):
            if f1[i] < 0.6:
                bar.set_color('#e74c3c')  # Rojo
            elif f1[i] < 0.8:
                bar.set_color('#f39c12')  # Amarillo
            else:
                bar.set_color('#2ecc71')  # Verde
        
        ax.set_yticks(x)
        ax.set_yticklabels(classes, fontsize=9)
        ax.set_xlabel('Score', fontsize=12)
        ax.set_title('M√©tricas por Clase (Ordenado por F1-Score)', fontsize=14, fontweight='bold')
        ax.legend(loc='lower right')
        ax.set_xlim(0, 1.0)
        ax.grid(axis='x', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_path / 'per_class_metrics.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ‚úÖ M√©tricas por clase: {viz_path / 'per_class_metrics.png'}")
    
    def _plot_per_crop_performance(self, crop_metrics, viz_path):
        """Visualiza accuracy por cultivo."""
        fig, ax = plt.subplots(figsize=(10, 6))
        
        crops = list(crop_metrics.keys())
        accuracies = [crop_metrics[crop]['accuracy'] for crop in crops]
        
        # Colores seg√∫n rendimiento
        colors = []
        for acc in accuracies:
            if acc >= 0.8:
                colors.append('#2ecc71')  # Verde
            elif acc >= 0.6:
                colors.append('#f39c12')  # Amarillo
            else:
                colors.append('#e74c3c')  # Rojo
        
        bars = ax.bar(crops, accuracies, color=colors, alpha=0.7, edgecolor='black')
        
        # Agregar valores encima de barras
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{acc:.2%}',
                   ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        # L√≠nea de referencia del promedio
        avg_acc = np.mean(accuracies)
        ax.axhline(y=avg_acc, color='red', linestyle='--', linewidth=2, 
                  label=f'Promedio: {avg_acc:.2%}', alpha=0.7)
        
        ax.set_ylabel('Accuracy', fontsize=12)
        ax.set_title('Accuracy por Cultivo', fontsize=14, fontweight='bold')
        ax.set_ylim(0, 1.0)
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_path / 'per_crop_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ‚úÖ Performance por cultivo: {viz_path / 'per_crop_performance.png'}")
    
    def _plot_healthy_vs_diseased(self, metrics, viz_path):
        """Visualiza an√°lisis binario: sano vs enfermo."""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
        # Matriz de confusi√≥n binaria
        cm_binary = metrics['cm_binary']
        labels = ['Diseased', 'Healthy']
        
        sns.heatmap(cm_binary, annot=True, fmt='d', cmap='RdYlGn', ax=ax1,
                   xticklabels=labels, yticklabels=labels,
                   cbar_kws={'label': 'Cantidad'})
        ax1.set_title('Confusion Matrix: Sano vs Enfermo', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Clase Real', fontsize=12)
        ax1.set_xlabel('Clase Predicha', fontsize=12)
        
        # M√©tricas
        metric_names = ['Accuracy', 'Precision\n(Healthy)', 'Recall\n(Healthy)']
        metric_values = [metrics['accuracy'], metrics['precision'], metrics['recall']]
        colors_metrics = ['#3498db', '#2ecc71', '#e74c3c']
        
        bars = ax2.bar(metric_names, metric_values, color=colors_metrics, alpha=0.7, edgecolor='black')
        
        for bar, val in zip(bars, metric_values):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{val:.2%}',
                    ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        ax2.set_ylim(0, 1.0)
        ax2.set_ylabel('Score', fontsize=12)
        ax2.set_title('M√©tricas Binarias', fontsize=14, fontweight='bold')
        ax2.grid(axis='y', alpha=0.3)
        
        # Agregar anotaciones de falsos
        if metrics['fn'] > 0:
            ax2.text(0.5, 0.15, f"‚ö†Ô∏è {metrics['fn']} Falsos Negativos\n(Enfermo ‚Üí Sano)",
                    transform=ax2.transAxes, ha='center', fontsize=10,
                    bbox=dict(boxstyle='round', facecolor='#e74c3c', alpha=0.3))
        
        plt.tight_layout()
        plt.savefig(viz_path / 'healthy_vs_diseased.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ‚úÖ An√°lisis binario: {viz_path / 'healthy_vs_diseased.png'}")
    
    def _generate_detailed_report(self, accuracy, loss, top3_acc, top5_acc,
                                  per_class_metrics, per_crop_metrics,
                                  healthy_diseased_metrics, top_confusions,
                                  class_names, viz_path):
        """Genera reporte de texto detallado."""
        report_path = viz_path / 'training_report.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            # Encabezado
            f.write("=" * 80 + "\n")
            f.write(" " * 20 + "REPORTE DE ENTRENAMIENTO\n")
            f.write("=" * 80 + "\n\n")
            
            # Metadata
            f.write(f"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Resoluci√≥n: {self.img_size[0]}x{self.img_size[1]}\n")
            f.write(f"Arquitectura: {'MobileNetV2 + Transfer Learning' if self.use_transfer_learning else 'CNN desde cero'}\n")
            f.write(f"N√∫mero de clases: {self.num_classes}\n\n")
            
            # M√©tricas globales
            f.write("=" * 80 + "\n")
            f.write("M√âTRICAS GLOBALES\n")
            f.write("=" * 80 + "\n")
            f.write(f"Accuracy:          {accuracy:.4f} ({accuracy*100:.2f}%)\n")
            f.write(f"Loss:              {loss:.4f}\n")
            f.write(f"Top-3 Accuracy:    {top3_acc:.4f} ({top3_acc*100:.2f}%)\n")
            f.write(f"Top-5 Accuracy:    {top5_acc:.4f} ({top5_acc*100:.2f}%)\n\n")
            
            # M√©tricas por clase
            f.write("=" * 80 + "\n")
            f.write("M√âTRICAS POR CLASE\n")
            f.write("=" * 80 + "\n")
            f.write(f"{'Clase':<35} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support'}\n")
            f.write("-" * 80 + "\n")
            
            for item in per_class_metrics:
                f.write(f"{item['class']:<35} {item['precision']:<12.4f} {item['recall']:<12.4f} "
                       f"{item['f1']:<12.4f} {item['support']}\n")
            
            # Per-crop
            f.write("\n" + "=" * 80 + "\n")
            f.write("ACCURACY POR CULTIVO\n")
            f.write("=" * 80 + "\n")
            
            for crop, metrics in sorted(per_crop_metrics.items(), key=lambda x: x[1]['accuracy'], reverse=True):
                acc = metrics['accuracy']
                f.write(f"{crop:<10} {acc:.4f} ({acc*100:.2f}%) - {metrics['correct']}/{metrics['total']} correctas\n")
            
            # Healthy vs Diseased
            f.write("\n" + "=" * 80 + "\n")
            f.write("AN√ÅLISIS: SANO VS ENFERMO\n")
            f.write("=" * 80 + "\n")
            hd = healthy_diseased_metrics
            f.write(f"Binary Accuracy: {hd['accuracy']:.4f} ({hd['accuracy']*100:.2f}%)\n")
            f.write(f"Precision:       {hd['precision']:.4f}\n")
            f.write(f"Recall:          {hd['recall']:.4f}\n\n")
            f.write(f"True Negatives:   {hd['tn']:4d} (Diseased ‚Üí Diseased)\n")
            f.write(f"False Positives:  {hd['fp']:4d} (Diseased ‚Üí Healthy)\n")
            f.write(f"False Negatives:  {hd['fn']:4d} (Healthy ‚Üí Diseased) ‚ö†Ô∏è\n")
            f.write(f"True Positives:   {hd['tp']:4d} (Healthy ‚Üí Healthy)\n")
            
            # Top confusiones
            f.write("\n" + "=" * 80 + "\n")
            f.write("TOP 10 CONFUSIONES M√ÅS FRECUENTES\n")
            f.write("=" * 80 + "\n")
            
            for idx, conf in enumerate(top_confusions, 1):
                f.write(f"{idx:2d}. {conf['true']:<30} ‚Üí {conf['pred']:<30} : {conf['count']:4d} veces\n")
            
            # Recomendaciones
            f.write("\n" + "=" * 80 + "\n")
            f.write("RECOMENDACIONES\n")
            f.write("=" * 80 + "\n")
            
            # Detectar sesgos
            max_f1 = max([m['f1'] for m in per_class_metrics])
            min_f1 = min([m['f1'] for m in per_class_metrics])
            
            if max_f1 - min_f1 > 0.3:
                worst_class = min(per_class_metrics, key=lambda x: x['f1'])
                f.write(f"‚ö†Ô∏è  Detectado desbalanceo: {worst_class['class']} tiene F1={worst_class['f1']:.2f}\n")
                f.write("üí° Sugerencia: Aplicar class weights o data augmentation espec√≠fico\n\n")
            
            if hd['fn'] > 10:
                f.write(f"‚ö†Ô∏è  CR√çTICO: {hd['fn']} falsos negativos (enfermo ‚Üí sano)\n")
                f.write("üí° Sugerencia: Ajustar umbral de decisi√≥n o mejorar recall en clases enfermas\n\n")
            
            if accuracy < 0.7:
                f.write("‚ö†Ô∏è  Accuracy global por debajo del 70%\n")
                f.write("üí° Sugerencias:\n")
                f.write("   - Aumentar epochs de fine-tuning\n")
                f.write("   - Verificar calidad del dataset\n")
                f.write("   - Considerar data augmentation m√°s agresivo\n\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("ARCHIVOS GENERADOS\n")
            f.write("=" * 80 + "\n")
            f.write(f"- {viz_path / 'confusion_matrix_detailed.png'}\n")
            f.write(f"- {viz_path / 'per_class_metrics.png'}\n")
            f.write(f"- {viz_path / 'per_crop_performance.png'}\n")
            f.write(f"- {viz_path / 'healthy_vs_diseased.png'}\n")
            f.write(f"- {viz_path / 'training_report.txt'}\n")
        
        print(f"\nüìÑ Reporte detallado guardado en: {report_path}")
    
    def plot_training_history(self):
        """Visualiza historial de entrenamiento."""
        if self.history is None:
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # Precisi√≥n
        axes[0].plot(self.history.history['accuracy'], label='Entrenamiento', linewidth=2)
        axes[0].plot(self.history.history['val_accuracy'], label='Validaci√≥n', linewidth=2)
        axes[0].set_title('Precisi√≥n del Modelo', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('√âpoca', fontsize=12)
        axes[0].set_ylabel('Precisi√≥n', fontsize=12)
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # P√©rdida
        axes[1].plot(self.history.history['loss'], label='Entrenamiento', linewidth=2)
        axes[1].plot(self.history.history['val_loss'], label='Validaci√≥n', linewidth=2)
        axes[1].set_title('P√©rdida del Modelo', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('√âpoca', fontsize=12)
        axes[1].set_ylabel('P√©rdida', fontsize=12)
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        viz_path = Path('models/visualizations')
        viz_path.mkdir(parents=True, exist_ok=True)
        plt.savefig(viz_path / 'training_history.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Historial guardado en: {viz_path / 'training_history.png'}")
    
    def save_model(self, filepath='models/fruit_classifier.keras', class_names=None):
        """Guarda el modelo y metadatos."""
        self.model.save(filepath)
        
        if class_names:
            class_mapping = {
                'class_names': class_names,
                'num_classes': len(class_names),
                'img_size': self.img_size
            }
            
            mapping_path = Path(filepath).parent / 'class_mapping.json'
            with open(mapping_path, 'w') as f:
                json.dump(class_mapping, f, indent=4)
            
            print(f"‚úÖ Modelo guardado: {filepath}")
            print(f"‚úÖ Mapeo guardado: {mapping_path}")


def main():
    """
    Funci√≥n principal de entrenamiento.
    
    IMPORTANTE: Si actualizas las clases del modelo, limpia el cache:
        python backend/utils/manage_cache.py
        Opci√≥n [2] - Limpiar cache
    """
    print("\n" + "=" * 70)
    print("üöÄ ENTRENAMIENTO DE CLASIFICADOR DE ENFERMEDADES")
    print("=" * 70)
    
    # ================================================================
    # CONFIGURACI√ìN
    # ================================================================
    RAW_DATASET = "dataset/raw"
    PROCESSED_DATASET = "dataset/processed"
    IMG_SIZE = (224, 224)  # Resoluci√≥n aumentada para mejor detecci√≥n de s√≠ntomas
    
    # Par√°metros de entrenamiento optimizados
    EPOCHS_PHASE1 = 15      # Entrenamiento inicial (capas Dense)
    EPOCHS_PHASE2 = 20      # Fine-tuning gradual (2 subfases) - Aumentado para mejor adaptaci√≥n
    BATCH_SIZE = 16         # Reducido de 32 para resoluci√≥n 224x224 (50,176 p√≠xeles vs 10,000)
    USE_TRANSFER_LEARNING = True
    DO_FINE_TUNING = True   # ‚úÖ Activado con estrategia gradual mejorada
    
    print("\n‚öôÔ∏è  CONFIGURACI√ìN:")
    print(f"  - Transfer Learning: {'‚úÖ MobileNetV2' if USE_TRANSFER_LEARNING else '‚ùå'}")
    print(f"  - Resoluci√≥n de Imagen: {IMG_SIZE[0]}x{IMG_SIZE[1]} ({IMG_SIZE[0]*IMG_SIZE[1]:,} p√≠xeles)")
    print(f"  - Batch Size: {BATCH_SIZE} (ajustado para resoluci√≥n alta)")
    print(f"  - √âpocas Fase 1 (clasificador): {EPOCHS_PHASE1}")
    if DO_FINE_TUNING and USE_TRANSFER_LEARNING:
        print(f"  - √âpocas Fase 2 (fine-tuning gradual): {EPOCHS_PHASE2}")
        print(f"    ‚Ä¢ Subfase 2a: ~{EPOCHS_PHASE2//2} epochs (capas 101-154: features complejas)")
        print(f"    ‚Ä¢ Subfase 2b: ~{EPOCHS_PHASE2 - EPOCHS_PHASE2//2} epochs (capas 51-154: +features intermedias)")
        print(f"    ‚Ä¢ Capas 0-50 permanecen congeladas (features b√°sicas de ImageNet)")
        print(f"  - Monitoreo: Sistema autom√°tico de detecci√≥n de √©xito/problemas activo")
    
    # Cargar datos desde cache
    cache = DataCache()
    
    # Configuraci√≥n con las 15 clases espec√≠ficas del dataset
    config = {
        'img_size': IMG_SIZE,
        'classes': [
            'Apple___Apple_scab',
            'Apple___Black_rot',
            'Apple___Cedar_apple_rust',
            'Apple___healthy',
            'Corn_(maize)___Common_rust_',
            'Corn_(maize)___healthy',
            'Corn_(maize)___Northern_Leaf_Blight',
            'Potato___Early_blight',
            'Potato___healthy',
            'Potato___Late_blight',
            'Tomato___Bacterial_spot',
            'Tomato___Early_blight',
            'Tomato___healthy',
            'Tomato___Late_blight',
            'Tomato___Leaf_Mold'
        ],
        'balance': False
    }
    
    print("\nüìÇ Cargando datos desde cache...")
    train_data = cache.load(RAW_DATASET, config, 'train')
    test_data = cache.load(RAW_DATASET, config, 'test')
    
    if not train_data or not test_data:
        print("\n‚ö†Ô∏è  Cache no encontrado. Preparando datos autom√°ticamente...")
        print("=" * 70)
        
        # Importar y ejecutar preparaci√≥n de datos
        from prepare_dataset import DatasetProcessor
        
        processor = DatasetProcessor(RAW_DATASET, PROCESSED_DATASET, IMG_SIZE)
        result = processor.prepare_optimized(use_cache=True, force_reprocess=False)
        
        if not result:
            print("\n‚ùå Error preparando datos. Verifica que el dataset exista en:")
            print(f"   {RAW_DATASET}/New Plant Diseases Dataset(Augmented)/train/")
            return
        
        # Cargar datos reci√©n preparados
        train_data = cache.load(RAW_DATASET, config, 'train')
        test_data = cache.load(RAW_DATASET, config, 'test')
        
        if not train_data or not test_data:
            print("\n‚ùå Error cargando datos preparados")
            return
    
    X_train, y_train, class_names = train_data
    X_test, y_test, _ = test_data
    
    num_classes = len(class_names)
    
    print(f"\n‚úÖ Datos cargados:")
    print(f"  - X_train: {X_train.shape}")
    print(f"  - X_test: {X_test.shape}")
    print(f"  - Clases: {class_names}")
    
    # Validaci√≥n de shapes
    expected_shape = (IMG_SIZE[0], IMG_SIZE[1], 3)
    actual_shape = X_train.shape[1:]
    if actual_shape != expected_shape:
        print(f"\n‚ö†Ô∏è  ALERTA: Shape mismatch detectado!")
        print(f"  - Esperado: {expected_shape}")
        print(f"  - Actual: {actual_shape}")
        print(f"  - Acci√≥n requerida: BORRAR backend/cache/*.pkl y re-ejecutar")
        return
    else:
        print(f"  ‚úÖ Validaci√≥n de shape exitosa: {actual_shape}")
    
    # Crear y construir modelo
    classifier = PlantDiseaseClassifier(
        img_size=IMG_SIZE,
        num_classes=num_classes,
        use_transfer_learning=USE_TRANSFER_LEARNING
    )
    
    classifier.build_model()
    
    # FASE 1: Entrenamiento inicial
    print("\n" + "=" * 60)
    print("FASE 1: ENTRENAMIENTO INICIAL")
    print("=" * 60)
    
    total_start = time.time()
    
    classifier.train_with_arrays(
        X_train, y_train,
        X_test, y_test,
        class_names=class_names,
        epochs=EPOCHS_PHASE1,
        batch_size=BATCH_SIZE
    )
    
    # FASE 2: Fine-tuning (opcional)
    if DO_FINE_TUNING and USE_TRANSFER_LEARNING:
        classifier.fine_tune(
            X_train, y_train,
            X_test, y_test,
            class_names=class_names,
            epochs=EPOCHS_PHASE2,
            batch_size=BATCH_SIZE
        )
    
    total_time = time.time() - total_start
    
    # Evaluaci√≥n final
    classifier.evaluate(X_test, y_test, class_names)
    
    # Visualizaciones
    classifier.plot_training_history()
    
    # Guardar modelo
    classifier.save_model('models/fruit_classifier.keras', class_names)
    
    # Resumen final
    print("\n" + "=" * 60)
    print("‚úÖ ENTRENAMIENTO COMPLETADO")
    print("=" * 60)
    print(f"\n‚è±Ô∏è  Tiempo total: {total_time/60:.2f} minutos")
    print(f"\nüìÅ Archivos generados:")
    print("  - models/best_model.keras")
    print("  - models/fruit_classifier.keras")
    print("  - models/class_mapping.json")
    print("  - models/visualizations/")
    
    print("\nüí° CARACTER√çSTICAS:")
    print("  ‚úÖ Preparaci√≥n autom√°tica: Detecta y prepara datos si es necesario")
    print("  ‚úÖ Cache PKL: Datos se guardan para reuso")
    print("  ‚úÖ Transfer Learning: Usa MobileNetV2 pre-entrenado")
    print("  ‚úÖ Alta Resoluci√≥n: 224x224 para mejor detecci√≥n de texturas y manchas")
    print("  ‚úÖ Data Augmentation: Previene overfitting")
    print("  ‚úÖ Fine-tuning Gradual: Descongelamiento progresivo en 2 fases")
    print("  ‚úÖ Monitoreo Inteligente: Detecta autom√°ticamente √©xito y problemas")
    print("  ‚úÖ Optimizado: Hiperpar√°metros balanceados")
    
    print("\nüéØ PR√ìXIMOS PASOS:")
    print("  1. Probar predicciones: python backend/scripts/predict.py <imagen>")
    print("  2. Iniciar API: python backend/app.py")
    print("  3. Re-entrenar: python backend/scripts/train.py")
    print("\n‚ö†Ô∏è  IMPORTANTE - Si cambias IMG_SIZE:")
    print("  1. BORRAR: backend/cache/*.pkl")
    print("  2. BORRAR: models/*.keras (modelos incompatibles)")
    print("  3. Re-ejecutar entrenamiento completo")


if __name__ == "__main__":
    main()
