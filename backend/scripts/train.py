"""
Script de entrenamiento del modelo de clasificaci√≥n de enfermedades.

Este script:
1. Prepara los datos autom√°ticamente (con cache PKL)
2. Entrena el modelo con Transfer Learning
3. Eval√∫a y guarda resultados

Uso:
    python backend/scripts/train.py
    
El sistema detecta autom√°ticamente si necesita preparar datos o puede
usar el cache existente.
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import sys
import time

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback
from sklearn.metrics import classification_report, confusion_matrix

# Agregar el directorio backend al path
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from utils.data_cache import DataCache

# Configurar para mejor rendimiento
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reducir logs de TensorFlow
tf.random.set_seed(42)
np.random.seed(42)

# Optimizaciones de TensorFlow
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print("‚úÖ GPU detectada y configurada")


class FineTuningMonitor(Callback):
    """
    Callback personalizado para monitorear se√±ales de √©xito y problemas durante fine-tuning.
    
    Se√±ales de √©xito:
    - ‚úÖ Val accuracy sube gradualmente
    - ‚úÖ Val loss baja sin oscilar mucho
    - ‚úÖ Gap train-val no es muy grande (<10%)
    
    Se√±ales de problemas:
    - ‚ùå Val loss explota ‚Üí LR demasiado alto
    - ‚ùå Overfitting severo (train 95%, val 50%) ‚Üí M√°s regularizaci√≥n
    - ‚ùå No mejora nada ‚Üí Posible problema en datos
    """
    
    def __init__(self, phase_name="Fine-tuning"):
        super().__init__()
        self.phase_name = phase_name
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.epochs_no_improve = 0
        self.val_loss_history = []
        
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        
        val_loss = logs.get('val_loss', 0)
        val_acc = logs.get('val_accuracy', 0)
        train_loss = logs.get('loss', 0)
        train_acc = logs.get('accuracy', 0)
        
        # Calcular gap train-val
        acc_gap = abs(train_acc - val_acc)
        
        # Guardar historial
        self.val_loss_history.append(val_loss)
        
        # Detectar volatilidad en val_loss
        if len(self.val_loss_history) >= 3:
            recent_losses = self.val_loss_history[-3:]
            volatility = max(recent_losses) - min(recent_losses)
        else:
            volatility = 0
        
        print(f"\nüìä [{self.phase_name}] Epoch {epoch + 1} - Monitoreo:")
        
        # SE√ëALES DE √âXITO
        success_signals = []
        
        if val_acc > self.best_val_acc:
            improvement = (val_acc - self.best_val_acc) * 100
            success_signals.append(f"‚úÖ Val accuracy mejora: +{improvement:.2f}%")
            self.best_val_acc = val_acc
            self.epochs_no_improve = 0
        
        if val_loss < self.best_val_loss:
            success_signals.append(f"‚úÖ Val loss baja: {val_loss:.4f}")
            self.best_val_loss = val_loss
        
        if acc_gap < 0.10:
            success_signals.append(f"‚úÖ Gap train-val saludable: {acc_gap*100:.1f}%")
        
        if volatility < 0.2:
            success_signals.append("‚úÖ Val loss estable (baja oscilaci√≥n)")
        
        # SE√ëALES DE PROBLEMAS
        problem_signals = []
        
        # Val loss explota
        if len(self.val_loss_history) >= 2:
            if val_loss > self.val_loss_history[-2] * 1.5:
                problem_signals.append("‚ùå ALERTA: Val loss explota - LR puede ser muy alto")
        
        # Overfitting severo
        if train_acc > 0.95 and val_acc < 0.70:
            problem_signals.append(f"‚ùå OVERFITTING SEVERO: train={train_acc:.1%}, val={val_acc:.1%}")
        elif acc_gap > 0.15:
            problem_signals.append(f"‚ö†Ô∏è  Gap train-val alto: {acc_gap*100:.1f}% (>15%)")
        
        # Estancamiento
        if val_acc <= self.best_val_acc:
            self.epochs_no_improve += 1
            if self.epochs_no_improve >= 5:
                problem_signals.append(f"‚ö†Ô∏è  Sin mejora por {self.epochs_no_improve} epochs")
        
        # Volatilidad alta
        if volatility > 0.3:
            problem_signals.append(f"‚ö†Ô∏è  Val loss oscila mucho: volatilidad={volatility:.3f}")
        
        # Imprimir se√±ales
        if success_signals:
            print("  " + "\n  ".join(success_signals))
        
        if problem_signals:
            print("  " + "\n  ".join(problem_signals))
        
        if not success_signals and not problem_signals:
            print("  üîµ Entrenamiento en progreso normal")
        
        # M√©tricas actuales
        print(f"  üìã M√©tricas: train_acc={train_acc:.1%}, val_acc={val_acc:.1%}, gap={acc_gap*100:.1f}%")


class PlantDiseaseClassifier:
    """Clasificador de enfermedades de plantas con Transfer Learning."""
    
    def __init__(self, img_size=(100, 100), num_classes=15, use_transfer_learning=True):
        """
        Inicializa el clasificador.
        
        Args:
            img_size: Tama√±o de las im√°genes
            num_classes: N√∫mero de clases (15 enfermedades espec√≠ficas por defecto)
            use_transfer_learning: Usar MobileNetV2 pre-entrenado
        """
        self.img_size = img_size
        self.num_classes = num_classes
        self.use_transfer_learning = use_transfer_learning
        self.model = None
        self.history = None
        self.cache = DataCache()
    
    def build_model(self):
        """Construye el modelo optimizado."""
        
        if self.use_transfer_learning:
            print("\nüöÄ Construyendo modelo con MobileNetV2 (Transfer Learning)")
            
            # Data augmentation
            data_augmentation = keras.Sequential([
                layers.RandomFlip("horizontal"),
                layers.RandomRotation(0.15),
                layers.RandomZoom(0.15),
                layers.RandomContrast(0.1),
            ], name="data_augmentation")
            
            # Modelo base pre-entrenado
            base_model = keras.applications.MobileNetV2(
                input_shape=(self.img_size[0], self.img_size[1], 3),
                include_top=False,
                weights='imagenet'
            )
            
            # Congelar base model
            base_model.trainable = False
            
            # Construir modelo con augmentation
            inputs = keras.Input(shape=(self.img_size[0], self.img_size[1], 3))
            x = data_augmentation(inputs)
            x = keras.applications.mobilenet_v2.preprocess_input(x)
            x = base_model(x, training=False)
            x = layers.GlobalAveragePooling2D()(x)
            x = layers.Dropout(0.3)(x)
            x = layers.Dense(256, activation='relu')(x)
            x = layers.Dropout(0.3)(x)
            outputs = layers.Dense(self.num_classes, activation='softmax')(x)
            
            model = keras.Model(inputs, outputs)
            self.base_model = base_model
            self.data_augmentation = data_augmentation
            
        else:
            print("\nüî® Construyendo CNN desde cero")
            
            model = models.Sequential([
                layers.Conv2D(32, (3, 3), activation='relu', 
                            input_shape=(self.img_size[0], self.img_size[1], 3)),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                layers.Conv2D(64, (3, 3), activation='relu'),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                layers.Conv2D(128, (3, 3), activation='relu'),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                layers.Flatten(),
                layers.Dense(256, activation='relu'),
                layers.Dropout(0.5),
                layers.Dense(self.num_classes, activation='softmax')
            ])
        
        # Compilar con learning rate ajustado
        initial_lr = 0.001 if self.use_transfer_learning else 0.0005
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=initial_lr),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        self.model = model
        
        print(f"\n‚úÖ Modelo construido: {model.count_params():,} par√°metros")
        return model
    
    def train_with_arrays(self, X_train, y_train, X_test, y_test, 
                         epochs=20, batch_size=64):
        """
        Entrena el modelo con arrays numpy (datos desde cache).
        
        Args:
            X_train: Array de im√°genes de entrenamiento
            y_train: Labels de entrenamiento (one-hot)
            X_test: Array de im√°genes de prueba
            y_test: Labels de prueba (one-hot)
            epochs: N√∫mero de √©pocas
            batch_size: Tama√±o del batch
            
        Returns:
            History object
        """
        print("\n" + "=" * 60)
        print("üéØ INICIANDO ENTRENAMIENTO")
        print("=" * 60)
        print(f"  - Muestras train: {len(X_train):,}")
        print(f"  - Muestras test: {len(X_test):,}")
        print(f"  - Batch size: {batch_size}")
        print(f"  - √âpocas: {epochs}")
        print()
        
        # Crear directorio para modelos
        Path('models').mkdir(exist_ok=True)
        
        # Callbacks optimizados para Fase 1
        callbacks = [
            EarlyStopping(
                monitor='val_accuracy',
                patience=7,
                restore_best_weights=True,
                verbose=1
            ),
            ModelCheckpoint(
                'models/best_model.keras',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,          # Decay agresivo para convergencia r√°pida
                patience=3,          # Reacci√≥n r√°pida a estancamiento
                min_lr=0.0001,       # M√≠nimo para Fase 1
                verbose=1
            )
        ]
        
        # Entrenar
        start_time = time.time()
        
        self.history = self.model.fit(
            X_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(X_test, y_test),
            callbacks=callbacks,
            verbose=1
        )
        
        training_time = time.time() - start_time
        
        print(f"\n‚è±Ô∏è  Tiempo de entrenamiento: {training_time/60:.2f} minutos")
        
        return self.history
    
    def fine_tune(self, X_train, y_train, X_test, y_test, 
                  epochs_phase2=15, batch_size=64):
        """
        Fine-tuning del modelo con descongelamiento gradual (solo si usa transfer learning).
        
        Estrategia basada en an√°lisis de features de MobileNetV2:
        - Capas 0-50:   Features b√°sicas (bordes, texturas) ‚Üí MANTENER CONGELADAS
        - Capas 51-100: Features intermedias (patrones) ‚Üí Fase 2b
        - Capas 101-154: Features complejas (objetos) ‚Üí Fase 2a (m√°s relevantes para hojas)
        
        Fase 2a: Descongela capas 101-154 (features complejas)
        Fase 2b: Descongela capas 51-154 (a√±ade features intermedias)
        
        Nota: BatchNormalization layers se manejan cuidadosamente con batch_size peque√±o.
        
        Args:
            X_train, y_train: Datos de entrenamiento
            X_test, y_test: Datos de prueba
            epochs_phase2: √âpocas totales de fine-tuning (se divide en 2 subfases)
            batch_size: Tama√±o del batch
        """
        if not self.use_transfer_learning:
            print("‚ö†Ô∏è  Fine-tuning solo disponible con transfer learning")
            return
        
        total_layers = len(self.base_model.layers)
        print(f"\nüìä Base model tiene {total_layers} capas totales")
        
        # ==================================================================
        # FASE 2a: Descongelamiento de features complejas (capas 101-154)
        # ==================================================================
        print("\n" + "=" * 60)
        print("üî• FASE 2a: FINE-TUNING - Features Complejas (Capas 101-154)")
        print("=" * 60)
        print("  üåø Objetivo: Adaptar detecci√≥n de objetos completos a morfolog√≠a de hojas")
        
        # Descongelar base model
        self.base_model.trainable = True
        
        # Estrategia: Descongelar solo capas 101-154 (features complejas)
        # Mantener congeladas 0-100 (features b√°sicas e intermedias)
        fine_tune_at = min(101, total_layers - 1)
        
        for i, layer in enumerate(self.base_model.layers):
            if i < fine_tune_at:
                layer.trainable = False
            else:
                # Proteger BatchNormalization con batch_size peque√±o
                if 'BatchNormalization' in layer.__class__.__name__ and batch_size < 16:
                    layer.trainable = False
                else:
                    layer.trainable = True
        
        trainable_layers = sum([1 for layer in self.base_model.layers if layer.trainable])
        frozen_bn = sum([1 for layer in self.base_model.layers[fine_tune_at:] 
                        if 'BatchNormalization' in layer.__class__.__name__ and not layer.trainable])
        
        print(f"  - Rango de capas: {fine_tune_at}-{total_layers} (features complejas)")
        print(f"  - Capas congeladas: 0-{fine_tune_at-1} (features b√°sicas/intermedias)")
        print(f"  - Capas descongeladas: {trainable_layers}")
        if frozen_bn > 0:
            print(f"  - BatchNorm protegidas: {frozen_bn} (batch_size={batch_size} < 16)")
        print(f"  - Learning Rate: 0.0001 (10x m√°s bajo que Fase 1)")
        print(f"  - LR Decay: factor=0.2, patience=5, min_lr=0.00001")
        
        # Recompilar con LR bajo
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.0001),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # Callbacks para Fase 2a (fine-tuning conservador)
        callbacks_2a = [
            FineTuningMonitor(phase_name="Fase 2a"),
            EarlyStopping(
                monitor='val_accuracy',
                patience=6,
                restore_best_weights=True,
                verbose=1
            ),
            ModelCheckpoint(
                'models/finetuned_phase2a.keras',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,          # Decay suave para evitar olvido catastr√≥fico
                patience=5,          # M√°s paciente en fine-tuning
                min_lr=0.00001,      # M√≠nimo para Fase 2
                verbose=1
            )
        ]
        
        # Entrenar Fase 2a
        epochs_2a = max(epochs_phase2 // 2, 7)  # M√≠nimo 7 epochs
        start_time_2a = time.time()
        
        history_2a = self.model.fit(
            X_train, y_train,
            batch_size=batch_size,
            epochs=epochs_2a,
            validation_data=(X_test, y_test),
            callbacks=callbacks_2a,
            verbose=1
        )
        
        time_2a = time.time() - start_time_2a
        print(f"\n‚è±Ô∏è  Tiempo Fase 2a: {time_2a/60:.2f} minutos")
        
        # ==================================================================
        # FASE 2b: Descongelamiento de features intermedias (capas 51-154)
        # ==================================================================
        print("\n" + "=" * 60)
        print("üî• FASE 2b: FINE-TUNING - Features Intermedias (Capas 51-154)")
        print("=" * 60)
        print("  üçÉ Objetivo: Adaptar detecci√≥n de patrones/formas a s√≠ntomas de enfermedades")
        
        # Estrategia: Descongelar capas 51-154 (features intermedias + complejas)
        # Mantener congeladas 0-50 (features b√°sicas: bordes, texturas)
        fine_tune_at_2b = min(51, total_layers - 1)
        
        for i, layer in enumerate(self.base_model.layers):
            if i < fine_tune_at_2b:
                layer.trainable = False
            else:
                # Proteger BatchNormalization con batch_size peque√±o
                if 'BatchNormalization' in layer.__class__.__name__ and batch_size < 16:
                    layer.trainable = False
                else:
                    layer.trainable = True
        
        trainable_layers_2b = sum([1 for layer in self.base_model.layers if layer.trainable])
        frozen_bn_2b = sum([1 for layer in self.base_model.layers[fine_tune_at_2b:] 
                           if 'BatchNormalization' in layer.__class__.__name__ and not layer.trainable])
        
        print(f"  - Rango de capas: {fine_tune_at_2b}-{total_layers} (features intermedias/complejas)")
        print(f"  - Capas congeladas: 0-{fine_tune_at_2b-1} (features b√°sicas preservadas)")
        print(f"  - Capas descongeladas: {trainable_layers_2b}")
        if frozen_bn_2b > 0:
            print(f"  - BatchNorm protegidas: {frozen_bn_2b} (batch_size={batch_size} < 16)")
        print(f"  - Learning Rate: 0.00005 (ultra-bajo para evitar catastrophic forgetting)")
        print(f"  - LR Decay: factor=0.2, patience=5, min_lr=0.00001")
        
        # Recompilar con LR muy bajo
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.00005),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # Callbacks para Fase 2b (fine-tuning ultra-conservador)
        callbacks_2b = [
            FineTuningMonitor(phase_name="Fase 2b"),
            EarlyStopping(
                monitor='val_accuracy',
                patience=5,
                restore_best_weights=True,
                verbose=1
            ),
            ModelCheckpoint(
                'models/best_model.keras',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,          # Decay suave para preservar features ImageNet
                patience=5,          # M√°s paciente con m√°s capas descongeladas
                min_lr=0.00001,      # M√≠nimo para Fase 2
                verbose=1
            )
        ]
        
        # Entrenar Fase 2b
        epochs_2b = epochs_phase2 - epochs_2a
        start_time_2b = time.time()
        
        history_2b = self.model.fit(
            X_train, y_train,
            batch_size=batch_size,
            epochs=epochs_2b,
            validation_data=(X_test, y_test),
            callbacks=callbacks_2b,
            verbose=1
        )
        
        time_2b = time.time() - start_time_2b
        print(f"\n‚è±Ô∏è  Tiempo Fase 2b: {time_2b/60:.2f} minutos")
        
        # Combinar historiales
        for key in self.history.history.keys():
            if key in history_2a.history:
                self.history.history[key].extend(history_2a.history[key])
            if key in history_2b.history:
                self.history.history[key].extend(history_2b.history[key])
        
        total_ft_time = time_2a + time_2b
        print(f"\n‚è±Ô∏è  Tiempo total de fine-tuning: {total_ft_time/60:.2f} minutos")
        print(f"\n‚úÖ Fine-tuning completado con {trainable_layers_2b} capas entrenables")
    
    def evaluate(self, X_test, y_test, class_names):
        """
        Eval√∫a el modelo.
        
        Args:
            X_test: Datos de prueba
            y_test: Labels de prueba (one-hot)
            class_names: Nombres de las clases
        """
        print("\n" + "=" * 60)
        print("üìä EVALUACI√ìN DEL MODELO")
        print("=" * 60)
        
        # Evaluar
        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=0)
        
        print(f"\n‚úÖ Resultados:")
        print(f"  - P√©rdida: {test_loss:.4f}")
        print(f"  - Precisi√≥n: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
        
        # Predicciones
        predictions = self.model.predict(X_test, verbose=0)
        predicted_classes = np.argmax(predictions, axis=1)
        true_classes = np.argmax(y_test, axis=1)
        
        # Reporte de clasificaci√≥n
        print("\nüìã Reporte de clasificaci√≥n:")
        print(classification_report(true_classes, predicted_classes, 
                                   target_names=class_names, digits=4))
        
        # Matriz de confusi√≥n
        cm = confusion_matrix(true_classes, predicted_classes)
        self._plot_confusion_matrix(cm, class_names)
        
        return test_loss, test_accuracy
    
    def _plot_confusion_matrix(self, cm, class_names):
        """Visualiza matriz de confusi√≥n."""
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=class_names,
                   yticklabels=class_names)
        plt.title('Matriz de Confusi√≥n', fontsize=16, fontweight='bold')
        plt.ylabel('Clase Real', fontsize=12)
        plt.xlabel('Clase Predicha', fontsize=12)
        plt.tight_layout()
        
        viz_path = Path('models/visualizations')
        viz_path.mkdir(parents=True, exist_ok=True)
        plt.savefig(viz_path / 'confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Matriz guardada en: {viz_path / 'confusion_matrix.png'}")
    
    def plot_training_history(self):
        """Visualiza historial de entrenamiento."""
        if self.history is None:
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # Precisi√≥n
        axes[0].plot(self.history.history['accuracy'], label='Entrenamiento', linewidth=2)
        axes[0].plot(self.history.history['val_accuracy'], label='Validaci√≥n', linewidth=2)
        axes[0].set_title('Precisi√≥n del Modelo', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('√âpoca', fontsize=12)
        axes[0].set_ylabel('Precisi√≥n', fontsize=12)
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # P√©rdida
        axes[1].plot(self.history.history['loss'], label='Entrenamiento', linewidth=2)
        axes[1].plot(self.history.history['val_loss'], label='Validaci√≥n', linewidth=2)
        axes[1].set_title('P√©rdida del Modelo', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('√âpoca', fontsize=12)
        axes[1].set_ylabel('P√©rdida', fontsize=12)
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        viz_path = Path('models/visualizations')
        viz_path.mkdir(parents=True, exist_ok=True)
        plt.savefig(viz_path / 'training_history.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Historial guardado en: {viz_path / 'training_history.png'}")
    
    def save_model(self, filepath='models/fruit_classifier.keras', class_names=None):
        """Guarda el modelo y metadatos."""
        self.model.save(filepath)
        
        if class_names:
            class_mapping = {
                'class_names': class_names,
                'num_classes': len(class_names),
                'img_size': self.img_size
            }
            
            mapping_path = Path(filepath).parent / 'class_mapping.json'
            with open(mapping_path, 'w') as f:
                json.dump(class_mapping, f, indent=4)
            
            print(f"‚úÖ Modelo guardado: {filepath}")
            print(f"‚úÖ Mapeo guardado: {mapping_path}")


def main():
    """
    Funci√≥n principal de entrenamiento.
    
    IMPORTANTE: Si actualizas las clases del modelo, limpia el cache:
        python backend/utils/manage_cache.py
        Opci√≥n [2] - Limpiar cache
    """
    print("\n" + "=" * 70)
    print("üöÄ ENTRENAMIENTO DE CLASIFICADOR DE ENFERMEDADES")
    print("=" * 70)
    
    # ================================================================
    # CONFIGURACI√ìN
    # ================================================================
    RAW_DATASET = "dataset/raw"
    PROCESSED_DATASET = "dataset/processed"
    IMG_SIZE = (100, 100)
    
    # Par√°metros de entrenamiento optimizados
    EPOCHS_PHASE1 = 15      # Entrenamiento inicial (capas Dense)
    EPOCHS_PHASE2 = 20      # Fine-tuning gradual (2 subfases) - Aumentado para mejor adaptaci√≥n
    BATCH_SIZE = 32         # Batch size para regularizaci√≥n
    USE_TRANSFER_LEARNING = True
    DO_FINE_TUNING = True   # ‚úÖ Activado con estrategia gradual mejorada
    
    print("\n‚öôÔ∏è  CONFIGURACI√ìN:")
    print(f"  - Transfer Learning: {'‚úÖ MobileNetV2' if USE_TRANSFER_LEARNING else '‚ùå'}")
    print(f"  - Batch Size: {BATCH_SIZE}")
    print(f"  - √âpocas Fase 1 (clasificador): {EPOCHS_PHASE1}")
    if DO_FINE_TUNING and USE_TRANSFER_LEARNING:
        print(f"  - √âpocas Fase 2 (fine-tuning gradual): {EPOCHS_PHASE2}")
        print(f"    ‚Ä¢ Subfase 2a: ~{EPOCHS_PHASE2//2} epochs (capas 101-154: features complejas)")
        print(f"    ‚Ä¢ Subfase 2b: ~{EPOCHS_PHASE2 - EPOCHS_PHASE2//2} epochs (capas 51-154: +features intermedias)")
        print(f"    ‚Ä¢ Capas 0-50 permanecen congeladas (features b√°sicas de ImageNet)")
        print(f"  - Monitoreo: Sistema autom√°tico de detecci√≥n de √©xito/problemas activo")
    
    # Cargar datos desde cache
    cache = DataCache()
    
    # Configuraci√≥n con las 15 clases espec√≠ficas del dataset
    config = {
        'img_size': IMG_SIZE,
        'classes': [
            'Apple___Apple_scab',
            'Apple___Black_rot',
            'Apple___Cedar_apple_rust',
            'Apple___healthy',
            'Corn_(maize)___Common_rust_',
            'Corn_(maize)___healthy',
            'Corn_(maize)___Northern_Leaf_Blight',
            'Potato___Early_blight',
            'Potato___healthy',
            'Potato___Late_blight',
            'Tomato___Bacterial_spot',
            'Tomato___Early_blight',
            'Tomato___healthy',
            'Tomato___Late_blight',
            'Tomato___Leaf_Mold'
        ],
        'balance': False
    }
    
    print("\nüìÇ Cargando datos desde cache...")
    train_data = cache.load(RAW_DATASET, config, 'train')
    test_data = cache.load(RAW_DATASET, config, 'test')
    
    if not train_data or not test_data:
        print("\n‚ö†Ô∏è  Cache no encontrado. Preparando datos autom√°ticamente...")
        print("=" * 70)
        
        # Importar y ejecutar preparaci√≥n de datos
        from prepare_dataset import DatasetProcessor
        
        processor = DatasetProcessor(RAW_DATASET, PROCESSED_DATASET, IMG_SIZE)
        result = processor.prepare_optimized(use_cache=True, force_reprocess=False)
        
        if not result:
            print("\n‚ùå Error preparando datos. Verifica que el dataset exista en:")
            print(f"   {RAW_DATASET}/New Plant Diseases Dataset(Augmented)/train/")
            return
        
        # Cargar datos reci√©n preparados
        train_data = cache.load(RAW_DATASET, config, 'train')
        test_data = cache.load(RAW_DATASET, config, 'test')
        
        if not train_data or not test_data:
            print("\n‚ùå Error cargando datos preparados")
            return
    
    X_train, y_train, class_names = train_data
    X_test, y_test, _ = test_data
    
    num_classes = len(class_names)
    
    print(f"\n‚úÖ Datos cargados:")
    print(f"  - X_train: {X_train.shape}")
    print(f"  - X_test: {X_test.shape}")
    print(f"  - Clases: {class_names}")
    
    # Crear y construir modelo
    classifier = PlantDiseaseClassifier(
        img_size=IMG_SIZE,
        num_classes=num_classes,
        use_transfer_learning=USE_TRANSFER_LEARNING
    )
    
    classifier.build_model()
    
    # FASE 1: Entrenamiento inicial
    print("\n" + "=" * 60)
    print("FASE 1: ENTRENAMIENTO INICIAL")
    print("=" * 60)
    
    total_start = time.time()
    
    classifier.train_with_arrays(
        X_train, y_train,
        X_test, y_test,
        epochs=EPOCHS_PHASE1,
        batch_size=BATCH_SIZE
    )
    
    # FASE 2: Fine-tuning (opcional)
    if DO_FINE_TUNING and USE_TRANSFER_LEARNING:
        classifier.fine_tune(
            X_train, y_train,
            X_test, y_test,
            epochs=EPOCHS_PHASE2,
            batch_size=BATCH_SIZE
        )
    
    total_time = time.time() - total_start
    
    # Evaluaci√≥n final
    classifier.evaluate(X_test, y_test, class_names)
    
    # Visualizaciones
    classifier.plot_training_history()
    
    # Guardar modelo
    classifier.save_model('models/fruit_classifier.keras', class_names)
    
    # Resumen final
    print("\n" + "=" * 60)
    print("‚úÖ ENTRENAMIENTO COMPLETADO")
    print("=" * 60)
    print(f"\n‚è±Ô∏è  Tiempo total: {total_time/60:.2f} minutos")
    print(f"\nüìÅ Archivos generados:")
    print("  - models/best_model.keras")
    print("  - models/fruit_classifier.keras")
    print("  - models/class_mapping.json")
    print("  - models/visualizations/")
    
    print("\nüí° CARACTER√çSTICAS:")
    print("  ‚úÖ Preparaci√≥n autom√°tica: Detecta y prepara datos si es necesario")
    print("  ‚úÖ Cache PKL: Datos se guardan para reuso")
    print("  ‚úÖ Transfer Learning: Usa MobileNetV2 pre-entrenado")
    print("  ‚úÖ Data Augmentation: Previene overfitting")
    print("  ‚úÖ Fine-tuning Gradual: Descongelamiento progresivo en 2 fases")
    print("  ‚úÖ Monitoreo Inteligente: Detecta autom√°ticamente √©xito y problemas")
    print("  ‚úÖ Optimizado: Hiperpar√°metros balanceados")
    
    print("\nüéØ PR√ìXIMOS PASOS:")
    print("  1. Probar predicciones: python backend/scripts/predict.py <imagen>")
    print("  2. Iniciar API: python backend/app.py")
    print("  3. Re-entrenar: python backend/scripts/train.py")


if __name__ == "__main__":
    main()
