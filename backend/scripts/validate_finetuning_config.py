"""
Script de validaci√≥n de las optimizaciones del fine-tuning (Paso 2.4).
Verifica que todos los par√°metros est√©n configurados correctamente.
"""

import sys
from pathlib import Path

# Agregar el directorio backend al path
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))


def validate_fine_tuning_config():
    """Valida la configuraci√≥n de fine-tuning optimizada."""
    
    print("=" * 80)
    print("VALIDACI√ìN DE OPTIMIZACIONES DE FINE-TUNING (PASO 2.4)")
    print("=" * 80)
    
    # Importar train.py para acceder a las configuraciones
    import importlib.util
    spec = importlib.util.spec_from_file_location("train_module", 
        backend_dir / "scripts" / "train.py")
    train_module = importlib.util.module_from_spec(spec)
    
    print("\n‚è≥ Leyendo configuraciones de train.py...")
    
    # Leer archivo para verificar par√°metros
    train_file = backend_dir / "scripts" / "train.py"
    with open(train_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ========== CHECKLIST ==========
    print("\n" + "‚ñº" * 80)
    print("CHECKLIST DE OPTIMIZACIONES")
    print("‚ñº" * 80)
    
    checks = []
    
    # 1. Verificar EPOCHS_PHASE2 = 10
    if "EPOCHS_PHASE2 = 10" in content:
        checks.append(("‚úÖ", "EPOCHS_PHASE2 reducido de 20 a 10"))
    else:
        checks.append(("‚ùå", "EPOCHS_PHASE2 NO est√° en 10"))
    
    # 2. Verificar Dropout = 0.5
    if "Dropout(0.5)" in content:
        dropout_count = content.count("Dropout(0.5)")
        checks.append(("‚úÖ", f"Dropout aumentado a 0.5 ({dropout_count} ocurrencias encontradas)"))
    else:
        checks.append(("‚ùå", "Dropout NO est√° en 0.5"))
    
    # 3. Verificar Learning Rate Fase 2a = 0.00005
    if "learning_rate=0.00005" in content:
        checks.append(("‚úÖ", "Learning Rate Fase 2a = 0.00005 (m√°s conservador)"))
    else:
        checks.append(("‚ùå", "Learning Rate Fase 2a NO est√° en 0.00005"))
    
    # 4. Verificar Learning Rate Fase 2b = 0.00001
    if "learning_rate=0.00001" in content:
        checks.append(("‚úÖ", "Learning Rate Fase 2b = 0.00001 (ultra-conservador)"))
    else:
        checks.append(("‚ùå", "Learning Rate Fase 2b NO est√° en 0.00001"))
    
    # 5. Verificar min_lr = 0.000001
    min_lr_count = content.count("min_lr=0.000001")
    if min_lr_count >= 2:
        checks.append(("‚úÖ", f"min_lr = 0.000001 en ReduceLROnPlateau ({min_lr_count} ocurrencias)"))
    else:
        checks.append(("‚ùå", "min_lr NO est√° configurado correctamente"))
    
    # 6. Verificar Early Stopping patience
    if "patience=5," in content or "patience=7," in content:
        patience_5 = content.count("patience=5,")
        patience_7 = content.count("patience=7,")
        checks.append(("‚úÖ", f"Early Stopping patience configurado (5: {patience_5} veces, 7: {patience_7} veces)"))
    else:
        checks.append(("‚ùå", "Early Stopping patience NO est√° configurado correctamente"))
    
    # 7. Verificar epochs_2a m√≠nimo reducido a 5
    if "max(epochs_phase2 // 2, 5)" in content:
        checks.append(("‚úÖ", "epochs_2a m√≠nimo reducido a 5 (antes era 7)"))
    else:
        checks.append(("‚ö†Ô∏è", "epochs_2a m√≠nimo NO est√° en 5 (podr√≠a estar en valor anterior)"))
    
    # Imprimir resultados
    for status, message in checks:
        print(f"  {status} {message}")
    
    # ========== RESUMEN DE CONFIGURACI√ìN ==========
    print("\n" + "‚ñº" * 80)
    print("RESUMEN DE CONFIGURACI√ìN OPTIMIZADA")
    print("‚ñº" * 80)
    
    print("\nüìã FASE 1: Entrenamiento Inicial")
    print("  ‚Ä¢ EPOCHS_PHASE1:       15")
    print("  ‚Ä¢ Learning Rate:       0.001")
    print("  ‚Ä¢ Dropout:             0.5 (aumentado desde 0.3)")
    print("  ‚Ä¢ Early Stop Patience: 7")
    print("  ‚Ä¢ Monitor:             val_accuracy")
    
    print("\nüìã FASE 2a: Fine-tuning Features Complejas (Capas 101-154)")
    print("  ‚Ä¢ Epochs:              ~5 (m√≠nimo)")
    print("  ‚Ä¢ Learning Rate:       0.00005 (reducido desde 0.0001)")
    print("  ‚Ä¢ Early Stop Patience: 5 (reducido desde 6)")
    print("  ‚Ä¢ ReduceLR min_lr:     0.000001 (reducido desde 0.00001)")
    print("  ‚Ä¢ Monitor:             val_accuracy")
    
    print("\nüìã FASE 2b: Fine-tuning Features Intermedias (Capas 51-154)")
    print("  ‚Ä¢ Epochs:              ~5 (resto de EPOCHS_PHASE2)")
    print("  ‚Ä¢ Learning Rate:       0.00001 (reducido desde 0.00005)")
    print("  ‚Ä¢ Early Stop Patience: 5")
    print("  ‚Ä¢ ReduceLR min_lr:     0.000001 (reducido desde 0.00001)")
    print("  ‚Ä¢ Monitor:             val_accuracy")
    
    print("\nüìã TOTAL EPOCHS:")
    print("  ‚Ä¢ Fase 1:              15 epochs")
    print("  ‚Ä¢ Fase 2 (2a + 2b):    10 epochs (reducido desde 20)")
    print("  ‚Ä¢ TOTAL M√ÅXIMO:        25 epochs (reducido desde 35)")
    
    # ========== JUSTIFICACI√ìN ==========
    print("\n" + "‚ñº" * 80)
    print("JUSTIFICACI√ìN DE CAMBIOS")
    print("‚ñº" * 80)
    
    print("\n‚úÖ EPOCHS_PHASE2: 20 ‚Üí 10")
    print("   Raz√≥n: Con desbalanceo corregido, converge m√°s r√°pido")
    print("   Beneficio: Evita overfitting en clases peque√±as")
    
    print("\n‚úÖ Learning Rates m√°s conservadores")
    print("   Fase 2a: 0.0001 ‚Üí 0.00005 (50% m√°s bajo)")
    print("   Fase 2b: 0.00005 ‚Üí 0.00001 (50% m√°s bajo)")
    print("   Raz√≥n: Proteger features pre-entrenadas de ImageNet")
    print("   Beneficio: Reduce riesgo de catastrophic forgetting")
    
    print("\n‚úÖ Early Stopping m√°s agresivo")
    print("   Patience: 6-7 ‚Üí 5-7 (m√°s agresivo en fine-tuning)")
    print("   Raz√≥n: Parar antes si no mejora")
    print("   Beneficio: Evita sobreentrenamiento y ahorra tiempo")
    
    print("\n‚úÖ Dropout aumentado")
    print("   Dropout rate: 0.3 ‚Üí 0.5 (66% m√°s alto)")
    print("   Raz√≥n: Prevenir memorizaci√≥n del desbalanceo")
    print("   Beneficio: Mejor generalizaci√≥n en clases minoritarias")
    
    # ========== IMPACTO ESPERADO ==========
    print("\n" + "‚ñº" * 80)
    print("IMPACTO ESPERADO")
    print("‚ñº" * 80)
    
    print("\nüéØ Resultado:")
    print("  ‚úÖ Entrenamiento m√°s estable")
    print("  ‚úÖ Menos overfitting en clases minoritarias")
    print("  ‚úÖ Mejor preservaci√≥n de features ImageNet")
    print("  ‚úÖ Convergencia m√°s r√°pida (~30% reducci√≥n en epochs totales)")
    print("  ‚úÖ Menor tiempo de entrenamiento (~20-25 min ahorrados)")
    
    # Verificar si todos los checks pasaron
    print("\n" + "=" * 80)
    failed_checks = [check for check in checks if check[0] == "‚ùå"]
    warning_checks = [check for check in checks if check[0] == "‚ö†Ô∏è"]
    
    if not failed_checks and not warning_checks:
        print("‚úÖ VALIDACI√ìN EXITOSA - TODAS LAS OPTIMIZACIONES APLICADAS")
    elif warning_checks and not failed_checks:
        print("‚ö†Ô∏è  VALIDACI√ìN CON ADVERTENCIAS - REVISAR CONFIGURACIONES")
    else:
        print("‚ùå VALIDACI√ìN FALLIDA - CORREGIR CONFIGURACIONES")
    print("=" * 80)
    
    return len(failed_checks) == 0


def main():
    """Ejecuta la validaci√≥n."""
    success = validate_fine_tuning_config()
    
    if success:
        print("\nüí° Pr√≥ximos pasos:")
        print("  1. Preparar datos: python backend/scripts/prepare_dataset.py")
        print("  2. Entrenar modelo: python backend/scripts/train.py")
        print("  3. Observar convergencia m√°s r√°pida y estable")
    else:
        print("\n‚ö†Ô∏è  Revisar y corregir las configuraciones marcadas con ‚ùå")
    
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
